\documentclass[a4paper, openany]{memoir}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage[linewidth=1pt]{mdframed}
\usepackage{multicol}
\usepackage{fancyvrb}
\usepackage{xcolor}

\usepackage[usestackEOL]{stackengine}
\stackMath

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\leftmark}
\fancyhead[RO]{\rightmark}
\fancyhead[RE, LO]{ToC}
\fancyfoot[LE, RO]{\thepage}
\fancyfoot[RE, LO]{Pete Gautam}

\renewcommand{\headrulewidth}{1.5pt}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}

\chapterstyle{thatcher}
\setcounter{chapter}{1}

\begin{document}
    \chapter{$\pi$-calculus}
    \section{Introduction to $\pi$-calculus}
    We saw that $\lambda$-calculus is a theory of \emph{sequential computation}. Here, we are interested in the results of functions applied to data. In $\pi$-calculus, we are interested in concurrent and parallel computation, communication between computing agents and continuous exchanges of input and output. There are many theories for \emph{concurrent computation} including $\pi$-calculus, and are described as \emph{process calculus} or algebra, where \emph{process} means an identifiable computing agent that can interact with the environment. So, $\pi$-calculus is a process calculus. Moreover, unlike other process calculi, it has \emph{mobility}- we can send a communication link (channel) as data that can be sent across another link.

    A \emph{process} is a computing agent that can interact with other processes by sending and receiving messages. Messages can be sent on \emph{channels} (or \emph{names}). There can be several senders and receivers on a single channel, but each message is sent by one process and received by one process. Communication is \emph{synchronous}- both sender and receiver block until the message is exchanged. There is no concept of \emph{location}. If we define a system by two processes in parallel, we don't care about whether they are on the same CPU or at different places in a distributed system. Nonetheless, these concepts can be used to extend $\pi$-calculus.

    Before defining the syntax, we will first consider $\pi$-calculus using some examples. These will involve numbers and arithmetic operations, which are not natively present in $\pi$-calculus, but still can be expressed by some $\pi$-calculus terms. This holds since $\pi$-calculus is a Turing-complete model of computation. 

    Consider the following term in $\pi$-calculus:
    \[a(x).a(y).\overline{a}\langle x + y \rangle.0\]
    In this term:
    \begin{itemize}
        \item the expression $a(x)$ means that we receive a message on some channel $a$, and refer to it using $x$- $x$ is like a function parameter, and is a bound variable.
        \item the dot means sequencing, and the sequences are left-to-right, i.e. first receive $x$ on $a$, then receive $y$ on $b$.
        \item $\overline{a} \langle x + y \rangle$ means that we are sending a message on channel $a$, and this is the result of the computation $x + y$.
        \item $0$ is the process that does nothing, and represents termination.
    \end{itemize}
    We can think of this term as some server- it receives 2 numbers from some client and sends back the sum of these two numbers.

    We can define a process that \emph{communicates} on $a$ in a dual way, i.e. a client for a server. So, consider the following term:
    \[\overline{a} \langle 2 \rangle.\overline{a} \langle 3 \rangle.a(z).P(z)\]
    In this case, we send the numbers 2 and 3 on the channel $a$ and await its output. Then, we process the message in some way using the call $P(z)$.

    We can now put the two process in parallel so that they can communicate with each on the channel $a$. This is done by \emph{reduction}:
    \begin{align*}
        a(x).a(y).\overline{a} \langle x + y \rangle.0 &\mid \overline{a} \langle 2 \rangle.\overline{a} \langle 3 \rangle.a(z).P(z) \\
        &\downarrow \\
        a(y).\overline{a} \langle 2 + y \rangle.0 &\mid \overline{a} \langle 3 \rangle.a(z).P(z) \\
        &\downarrow \\
        \overline{a} \langle 2 + 3 \rangle.0 &\mid a(z).P(z) \\
        &\downarrow \\
        \overline{a} \langle 5 \rangle.0 &\mid a(z).P(z) \\
        &\downarrow \\
        0 &\mid P(5)
    \end{align*}

    We will now look at some more operations in $\pi$-calculus. The choice operation $+$ gives us a choice between two different ways of communication. For instance, consider the following term:
    \[a(x).a(y).\overline{a} \langle x + y \rangle.0 + b(x).b\langle x^2 \rangle.0\]
    We can think of this as the server providing multiple functionalities, and we can choose the one we want based on the channel name ($a$ or $b$). The choice is non-deterministic and part of reduction. This means that the expression 
    \[a(x).a(y).\overline{a} \langle x + y \rangle.0 + b(x).b\langle x^2 \rangle.0 \mid \overline{a} \langle 2 \rangle.\overline{a} \langle 3 \rangle.a(z).P(z)\]
    reduces in one step to
    \[a(y).\overline{a} \langle 2 + y \rangle.0 \mid \overline{a} \langle 3 \rangle.a(z).P(z)\]

    We illustrate the choice operation for the following process:
    \begin{align*}
        a(x).a(y).\overline{a} \langle x + y \rangle.0 + b(x).\overline{b}\langle x^2 \rangle.0 &\mid \overline{b} \langle 3 \rangle.b(z).P(z) \\
        &\downarrow \\
        \overline{b}\langle3^2\rangle.0 &\mid b(z).P(z) \\
        &\downarrow \\
        \overline{b}\langle 9 \rangle.0 &\mid b(z).P(z) \\
        &\downarrow \\
        0 &\mid P(9)
    \end{align*}

    We can add recursive definitions to the syntax. For instance, consider the following term:
    \[A = b(x).\overline{b}\langle x^2 \rangle.A\]
    Then, the following illustrates how we can reduce recursively:
    \begin{align*}
        b(x).\overline{b} \langle x^2 \rangle.A &\mid \overline{b} \langle 2 \rangle.b(z).\overline{b} \langle 3 \rangle.b(w).P(z, w) \\
        &\downarrow \\
        \overline{b}(2^2).A &\mid b(z).\overline{b} \langle 3 \rangle.b(w).P(z, w) \\
        &\downarrow \\
        A &\mid \overline{b} \langle 3 \rangle.b(w).P(4, w) \\
        &= \\
        b(x).\overline{b} \langle x^2 \rangle.A &\mid \overline{b} \langle 3 \rangle.b(w).P(4, w) \\
        &\downarrow \\ 
        \overline{b} \langle 3^2 \rangle.A &\mid b(w).P(4, w) \\
        &\downarrow \\
        A &\mid P(4, 9)
    \end{align*}
    
    Instead of adding recursion, we can introduce \emph{replication} to get a simpler theory. For a process $P$, the term $!P$ represents a potentially unlimited number of copies of $P$ in parallel. We can pull another copy out whenever we need to. For instance, the process
    \[!(b(x).\overline{b} \langle x^2 \rangle.0) \mid \overline{b} \langle 2 \rangle.b(z).\overline{b} \langle 3 \rangle.b(w).P(z, w)\]
    is equal to
    \[b(x).\overline{b} \langle x^2 \rangle.0 \mid !(b(x).\overline{b} \langle x^2 \rangle.0) \mid \overline{b} \langle 2 \rangle.b(z).\overline{b} \langle 3 \rangle.b(w).P(z, w)\]
    which reduces (eventually) to 
    \[0 \mid !(b(x).\overline{b} \langle x^2 \rangle.0) \mid \overline{b} \langle 3 \rangle.b(w).P(4, w)\]
    At this point, we can pull out another copy, to get the equivalent process
    \[0 \mid b(x).\overline{b} \langle x^2 \rangle.0 \mid !(b(x).\overline{b} \langle x^2 \rangle.0) \mid \overline{b} \langle 3 \rangle.b(w).P(4, w)\]
    and continue reduction.

    The $\pi$-calculus is based on non-determinism, which can lead to some issues. For instance, there can be several senders and receivers on the same channel in parallel. Consider the following term:
    \[a(x).a(y).\overline{a}\langle x + y \rangle.0 \mid \overline{a} \langle 2 \rangle.\overline{a} \langle 3 \rangle.a(z).P(z) \mid \overline{a} \langle 4 \rangle.\overline{a} \langle 5 \rangle.a(w).Q(w)\]
    Then, this process can reduce to either 
    \[a(y).\overline{a}\langle 2 + y \rangle.0 \mid \overline{a} \langle 3 \rangle.a(z).P(z) \mid \overline{a} \langle 4 \rangle.\overline{a} \langle 5 \rangle.a(w).Q(w)\]
    or 
    \[a(y).\overline{a}\langle 3 + y \rangle.0 \mid \overline{a} \langle 2 \rangle.\overline{a} \langle 3 \rangle.a(z).P(z) \mid \overline{a} \langle 3 \rangle.a(w).Q(w)\]
    Now, for the process to not get stuck, we need to ensure that the channel $a$ receives the second message from the same channel.

    To avoid the issue above of getting stuck, we can make use of the restriction operator $\nu$. The restriction operator defines a local scope for a channel. It is a binder, and we can use $\alpha$-equivalence to rename a local channel, e.g. the channel
    \[(\nu \ a)\textbf{(}a(x).a(y).\overline{a} \langle x + y \rangle.0 \mid \overline{a} \langle 2 \rangle.\overline{a} \langle 3 \rangle.a(z).P(z) \textbf{)}\]
    is $\alpha$-equivalent to
    \[(\nu \ b)\textbf{(}b(x).b(y).\overline{b} \langle x + y \rangle.0 \mid \overline{b} \langle 2 \rangle.\overline{b} \langle 3 \rangle.b(z).P(z) \textbf{)}\]
    Note that the channel also leads to a bound variable, i.e. $x$ is bound in $(\nu \ x)\textbf{(}\dots\textbf{)}$. Bound variables can be renamed using $\alpha$-equivalence.

    Using restriction, we can share private channels to ensure complete interaction. This is done using scope extrusion, which is shown in the reduction below:
    \begin{align*}
        r(a).a(x).a(y).\overline{a} \langle x + y \rangle.0 &\mid (\nu \ b)\textbf{(}\overline{r}\langle b \rangle.\overline{b} \langle 2 \rangle.\overline{b} \langle 3 \rangle.b(z).P(z) \textbf{)} \\
        &= \\
        (\nu \ b)\textbf{(}r(a).a(x).a(y).\overline{a} \langle x + y \rangle.0 &\mid \overline{r}\langle b \rangle.\overline{b} \langle 2 \rangle.\overline{b} \langle 3 \rangle.b(z).P(z) \textbf{)} \\
        &\downarrow \\
        (\nu \ b)\textbf{(}b(x).b(y).\overline{b} \langle x + y \rangle.0 &\mid \overline{b} \langle 2 \rangle.\overline{b} \langle 3 \rangle.b(z).P(z) \textbf{)} 
    \end{align*}
    At the first step, we expand the scope to include both processes, and is called \emph{scope expansion}. Then, we send in the channel that will be used in communication. The two steps are referred to as \emph{scope extrusion}. The output $\overline{r} \langle b \rangle$ carries the scope of $b$ with it, which allows us to create a private channel for the rest of the communication.
    
    We will now illustrate how we can combine replication and restriction:
    \begin{align*}
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) &\mid (\nu \ b) \textbf{(} \overline{r} \langle b \rangle.\overline{b} \langle 2 \rangle.b(z).P(z) \textbf{)} \\
        &= \\
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) \mid r(a).a(x).\overline{a} \langle x^2 \rangle.0 \mid &\mid (\nu \ b) \textbf{(} \overline{r} \langle b \rangle.\overline{b} \langle 2 \rangle.b(z).P(z) \textbf{)} \\
        &= \\
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) \mid (\nu \ b) \textbf{(} r(a).a(x).\overline{a} \langle x^2 \rangle.0 \mid &\mid \overline{r} \langle b \rangle.\overline{b} \langle 2 \rangle.b(z).P(z) \textbf{)} \\
        &\downarrow \\
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) \mid (\nu \ b) \textbf{(} b(x).\overline{b} \langle x^2 \rangle.0 &\mid \overline{b} \langle 2 \rangle.b(z).P(z) \textbf{)} \\
        &\downarrow \\
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) \mid (\nu \ b) \textbf{(} \overline{b} \langle 2^2 \rangle.0 &\mid b(z).P(z) \textbf{)} \\
        &\downarrow \\
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) \mid (\nu \ b) \textbf{(} 0 &\mid P(4) \textbf{)} \\
        &= \\
        !(r(a).a(x).\overline{a} \langle x^2 \rangle.0) \mid 0 &\mid P(4) 
    \end{align*}

    The ability to send a channel as a message is called \emph{mobility}. This was the key advance of $\pi$-calculus in comparison with previous process calculi such as CCS and CSP. $\pi$-calculus is called a theory of mobile processes, although actually it is the channels that are mobile. Moving a process around a network can be modelled- instead of process moving, a channel that gives access to it can move. There are extensions of $\pi$-calculus in which \emph{processes} can be sent as messages. This is called \emph{higher-order} communication.

    We will now define $\pi$-calculus formally. Let $x, y, \dots$ denote channel \emph{names} or \emph{variables}, and $P, Q, \dots$ denote \emph{processes}. Then, the syntax of processes is defined by the BNF below:
    \begin{align*}
        P, Q &:= 0 & \textrm{terminated process} \\
        &\mid x(y).P & \textrm{input/receive} \\
        &\mid \overline{x}(y).P & \textrm{output/send} \\
        &\mid \tau.P & \textrm{silent action} \\
        &\mid P + Q & \textrm{choice} \\
        &\mid (\nu \ x) P & \textrm{scope/restriction} \\
        &\mid !P & \textrm{replication} \\
        &\mid P \mid Q & \textrm{parallel composition}
    \end{align*}
    Other process constructions, like conditions, case, etc. can be added to the syntax of processes, but they are not in the core.

    Now, we want to define the \emph{semantics} by \emph{reduction relation} on processes. The main rule of communication is:
    \[a(x).P \mid \overline{a} \langle y \rangle.Q \to P[x := y] \mid Q\]
    We want to be able to apply this rule in the presence of other parallel processes, i.e. in bigger \emph{contexts}, e.g.
    \[a(x).P \mid \textbf{R} \mid \overline{a} \langle y \rangle.Q \to P[x := y] \mid \textbf{R} \mid Q\]
    We have to dom something about the fact that communicating parts of the process might not be written next to each other. Syntax is all in a line, but we want to think of parallel processes in a space where any process can interact with any other.

    To do so, we define \emph{structural congruence} ($\equiv$) on processes; it compensates for inessential syntactic details, as well as defining some important aspects of the behaviour of processes. It is defined by several axioms, and is also a \emph{congruence}, meaning that it is preserved by all the syntactic constructs, i.e. we can apply reduction in bigger contexts, and it is an equivalence relation. In particular, congruence means that:
    \begin{itemize}
        \item if $P \equiv Q$, then $P \mid R \equiv Q \mid R$;
        \item if $P \equiv Q$, then $P + R \equiv Q + R$;
        \item if $P \equiv Q$, then $x(y).P \equiv x(y).Q$;
        \item if $P \equiv Q$, then $\overline{x}\langle y\rangle.P \equiv \overline{x}\langle y\rangle.Q$;
        \item if $P \equiv Q$, then $(\nu \ x)P \equiv (\nu \ x)Q$; and
        \item if $P \equiv Q$, then $!P \equiv \ !Q$.
    \end{itemize}
    And, equivalence relation means that:
    \begin{itemize}
        \item $P \equiv P$;
        \item if $P \equiv Q$ then $Q \equiv P$; and 
        \item if $P \equiv Q$ and $Q \equiv R$, then $P \equiv R$.
    \end{itemize}
    The full definition of structural congruence is given below:
    \begin{align*}
        P \mid Q &\equiv Q \mid P & \textrm{parallel is commutative} \\
        P \mid (Q \mid R) &\equiv (P \mid Q) \mid R & \textrm{parallel is associative} \\
        P \mid 0 &\equiv P & \textrm{garbage collection} \\
        P + Q &\equiv Q + P & \textrm{choice is commutative} \\
        P + (Q \mid R) &\equiv (P + Q) + R & \textrm{choice is associative} \\
        P + 0 &\equiv P & \textrm{garbage collection} \\
        (\nu \ x)(\nu \ y)P &\equiv (\nu \ y)(\nu \ x)P & \textrm{reordering } \nu \\
        (\nu \ x)0 &\equiv 0 & \textrm{garbage collection} \\
        !P &\equiv P \mid !P & \textrm{replication} \\
        P \mid (\nu \ x) Q &\equiv (\nu \ x)(P \mid Q) \textrm{ if } x \not\in FV(P) & \textrm{scope expansion}
    \end{align*}
    It also includes $\alpha$-equivalence. Informally, the definition states that:
    \begin{itemize}
        \item we can ignore the order of processes in parallel and choice constructs;
        \item we do not need to write brackets in parallel and choice constructs;
        \item we can reorder $\nu$ binders;
        \item we can remove $0$ and $(\nu \  x) 0$ from parallel and choice constructs;
        \item we can pull out a copy of $P$ from $!P$ if necessary; and
        \item we can expand the scope of $(\nu \ x)$ whenever necessary, and rename $x$ if we need to, so as to avoid a variable capture.
    \end{itemize}

    We can now define the reduction relation. Before doing so, there are two things to consider:
    \begin{itemize}
        \item substitution is defined in a similar way to $\lambda$-calculus, but we only substitute variables; and
        \item bound variables can be renamed if necessary to avoid variable capture. This can be done using Barendregt convention.
    \end{itemize}
    Now, these are the reduction axioms:
    \begin{align*}
        (\overline{a} \langle x \rangle.P + \dots) \mid (a \langle y \rangle.Q + \dots) &\to P \mid Q[ y:= x] & \textrm{RCom} \\
        \tau.P + \dots &\to P & \textrm{RTau}.
    \end{align*}
    The first one allows us to substitute via communication, while the second one takes the $\tau$ choice (which can always be chosen). We extend these using the following inference rules:
    \[\frac{P \to Q}{(\nu \ x)P \to (\nu \ x)Q} \textrm{RNew} \qquad \frac{P \to Q}{P \mid R \to Q \mid R} \textrm{RPar}\]
    \[\frac{P' \equiv P \qquad P \to R \qquad Q \equiv Q'}{P' \equiv Q'} \textrm{RStruct}\]
    \newpage

    \section{Modelling and Computation}
    There are two directions we can go with $\pi$-calculus:
    \begin{itemize}
        \item We can model systems by adding data and computation (e.g. integers and operations) when we need them. In this view, $\pi$-calculus is a concurrency layer on top of an assumed computational base.
        \item We can study $\pi$-calculus as a foundation for all computation, like $\lambda$-calculus.
    \end{itemize}
    
    \subsection{Modelling in $\pi$-calculus}
    The mobile phones example is a classic $\pi$-calculus example that illustrates \emph{dynamic} communication topology. Note that the key feature of $\pi$-calculus is mobility.

    Here, we have a car, two transmitters and a controller. At any time, the car communicates with only one of the transmitters. The controller tells a transmitter to \textit{lose} or \textit{gain} connection to the car.

    % TODO: Add figure

    We will use parametrised recursive program definitions instead of replication and send/receive with multiple messages. We will later see how to translate recursive definitions into replication. 
    
    \textit{Trans} is parametrised by the channels it shares with \textit{Control}, which are \textit{lose} and \textit{gain}. On the other hand, \textit{Car} is parametrised by \textit{talk} and \textit{switch}. It can either \textit{talk} or obey an instruction to \textit{lose} the connection.

    \textit{IdTrans} is the idle transmitter. It is parametrised by the channels it shares with \textit{Control}, i.e. \textit{lose} and \textit{gain}. It can obey an instruction to gain a connection.

    Putting this all together, we get:
    \begin{align*}
        \textit{Trans}(\textit{talk}, \textit{switch}, \textit{gain}, \textit{lose}) &= \textit{talk}().\textit{Trans}(\textit{talk}, \textit{switch}, \textit{gain}, \textit{lose}) \\
        &+ \textit{lose}(t, s).\overline{\textit{switch}}\langle t, s \rangle.\textit{IdTrans}(\textit{gain}, \textit{lose}) \\
        \textit{IdTrans}(\textit{gain}, \textit{lose}) &= \textit{gain}(t, s).\textit{Trans}(t, s, \textit{gain}, \textit{lose})
    \end{align*}

    \textit{Control} will either be $\textit{Control}_1$ or $\textit{Control}_2$. \textit{Control} can tell one transmitter to lose a connection and the other to gain a connection. We ignore how it decides this. So, we have:
    \begin{align*}
        \textit{Control}_1 &= \overline{\textit{lose}_1} \langle \textit{talk}_2, \textit{switch}_2 \rangle.\overline{\textit{gain}_2} \langle \textit{talk}_2, \textit{switch}_2 \rangle.\textit{Control}_2 \\
        \textit{Control}_2 &= \overline{\textit{lose}_2} \langle \textit{talk}_1, \textit{switch}_1 \rangle.\overline{\textit{gain}_1} \langle \textit{talk}_1, \textit{switch}_1 \rangle.\textit{Control}_1
    \end{align*}
    We treat $\textit{lose}_i$, $\textit{talk}_i$ and $\textit{switch}_i$ channels as global variables for $i \in \{1, 2\}$. We will define a system in which they are declared in a scope that includes \textit{Control}.

    Now, the \textit{Car} can either \textit{talk} or \textit{switch} to a new pair of channels. We would expect \textit{switch} to have priority over \textit{talk}, but this is not enforced. The definition of \textit{Car} is therefore the following:
    \[\textit{Car}(\textit{talk}, \textit{switch}) = \overline{\textit{talk}}().\textit{Car} (\textit{talk}, \textit{switch}) + \textit{switch} (t, s).\textit{Car}(t, s)\]

    Finally, we define the \textit{System}, with the starting state $\textit{Trans}_1$. This is given by:
    \begin{align*}
        \textit{System}_1 &= (\nu \ \textit{talk}_1, \textit{switch}_1, \textit{gain}_1, \textit{lose}_1, \textit{talk}_2, \textit{switch}_2, \textit{gain}_2, \textit{lose}_2) \\
        &\qquad (\textit{Car}(\textit{talk}_1, \textit{switch}_1) \mid \textit{Trans}_1 \mid \textit{IdTrans}_2 \mid \textit{Control}_1)
    \end{align*}
    where for $i \in \{1, 2\}$,
    \begin{align*}
        \textit{Trans}_i &= \textit{Trans}(\textit{talk}_i, \textit{switch}_i, \textit{gain}_i, \textit{lose}_i) \\
        \textit{IdTrans}_i &= \textit{IdTrans}(\textit{gain}_i, \textit{lose}_i) 
    \end{align*}

    Using this definition, we can reduce $\textit{System}_1$ reduces to $\textit{System}_2$:
    % TODO: Reduce

    % TODO: Modify the definition so that the new \textit{switch} and \textit{talk} channels are created every time the active \textit{Trans} changes.

    We defined \textit{Car} as follows:
    \[\textit{Car}(\textit{talk}, \textit{switch}) = \overline{\textit{talk}}().\textit{Car} (\textit{talk}, \textit{switch}) + \textit{switch} (t, s).\textit{Car}(t, s)\]
    This is recursive parametrised definition of \textit{Car}. We can write it using replication using a replicated process that receives \textit{talk} and \textit{switch} on a channel called \textit{start}. We first define \textit{NewCar}:
    \[\textit{NewCar} = !(\textit{start}(t, s).(
        \overline{\textit{talk}}.\overline{\textit{start}}\langle t, s \rangle.0 + 
        \textit{switch}(x, y).\overline{start} \langle x, y \rangle.0
    ))\]
    We can now replace $\textit{Car}(\textit{talk}_1, \textit{switch}_1)$ with:
    \[(\nu \ \textit{start})(\overline{\textit{start}} \langle \textit{talk}_1, \textit{switch}_1 \rangle.0 \mid \textit{NewCar})\]

    That way, we can reduce the following process:
    \[\overline{\textit{start}} \langle \textit{talk}_1, \textit{switch}_1 \rangle.0 \mid \textit{NewCar} \mid \textit{talk}_1().0\]
    % TODO
    \[\overline{\textit{start}} \langle \textit{talk}_1, \textit{switch}_1 \rangle.0 \mid \textit{NewCar}\]

    \subsection{Computation in $\pi$-calculus}
    In this section, we discuss how we can represent booleans and natural numbers in $\pi$-calculus.

    In $\pi$-calculus, we can represent a boolean value by a process that is given two channels and communicates on one of them. This is similar to the $\lambda$-calculus representation, but we now also need to specify a channel for interaction with the boolean; this is called its \emph{location}. We have
    \[\textit{True}(a) = a(t, f).\overline{t}\langle\rangle.0 \qquad \textit{False}(a) = a(t, f).\overline{f}\langle\rangle.0\]
    We can abbreviate definitions by omitting the $0$ at the end and empty braces. For instance, we have
    \[\textit{True}(a) = a(t, f).\overline{t}\]

    We can define a choice between processes $P$ and $Q$ on a boolean located at channel $a$. This is given by:
    \[\textit{Cond}(P, Q)(a) = (\nu \ t, f)\overline{a} \langle t, f \rangle.(t.P + f.Q)\]
    We illustrate how this works with an example. In particular, consider the following reduction:
    \[\textit{True}(a) \mid \textit{Cond}(P, Q)(a)\]
    % TODO: Reduce to P

    Similarly,
    \[\textit{False}(a) \mid \textit{Cond}(P, Q)(a) \to^* Q\]

    Next, we can define the process $\textit{Not}(a, b)$, where $b$ is where the boolean currently is located, and $a$ is where it will be after the negation process. This process is given by:
    \[\textit{Not}(a, b) = (\nu \ t, f)(\overline{b} \langle t, f \rangle.(
        t.\textit{False}(a) + f.\textit{True}(a)
    ))\]
    We illustrate this with the following reduction:
    \[\textit{Not}(a, b) \mid \textit{True}(b)\]
    % TODO: Reduce to False(a)

    We now define the process $\texttt{And}(a, b, c)$, which takes in two booleans at $b$ and $c$ and produces a boolean at $a$. This is given by:
    \[\textit{And}(a, b, c) = (\nu t, f)(\overline{b} \langle t, f \rangle.(
        f.\textit{False}(a) + t.\overline{c} \langle t, f \rangle.(
            f.\textit{False}(a) + t.\textit{True}(b)
        )
    ))\]
    We illustrate this with the following reduction:
    \[\textit{And}(a, b, c) \mid \textit{True}(b) \mid \textit{False}(c)\]
    % TODO: Reduce to True(a)

    We will now use a similar idea to represent natural numbers. It is more direct than in $\lambda$-calculus since we do not need \textit{pair}. We define it recursively, with
    \begin{align*}
        Z(n_0) &= n_0(z, s).\overline{z} \\
        S(n_k, N(n_{k-1})) &= (\nu \ n_{k-1})(n_k(z, s).\overline{s} \langle n_{k-1} \rangle \mid N(n_{k-1}))
    \end{align*}
    This way, we represent the value $\textit{One}$ as follows:
    \[\textit{One}(n_1) = S(n_1, Z(n_0)) = (\nu \ n_1)(n_0(z, s) \overline{s} \langle n_1 \rangle \mid n_1(z, s).\overline{z})\]
    
    Like \textit{Cond} for booleans, we can define a case-analysis process for natural numbers, as follows:
    \[\textit{Case}(P, Q)(a, n) = (\nu \ z, s)\overline{a} \langle z, s \rangle.(z.P + s(n).Q(n)).\]
    The process \textit{Cases} interacts with a number located at $a$. If it is zero, it will next perform $P$; if it is instead $S(n)$, then it will continue as $Q(n)$. Using this, we can define the \textit{IsZero} process:
    \[\textit{IsZero}(a, n) = \textit{Cases}(\textit{True}(a), \textit{False}(a))(a, n).\]
    % TODO: IsZero(a, n) | Zero \to^* True(a)
    % TODO: IsZero(a, n) | One(n) \to^* False(a) | G (garbage; cannot interact with anything)

    We will now define the \textit{even} process in $\pi$-calculus. Formally, the \textit{even} function can be defined as follows:
    \begin{align*}
        \textit{even}(Z) &= \textit{true} \\
        \textit{even}(S(n)) &= \textit{not}(\textit{even}(n))
    \end{align*}
    In $\pi$-calculus, we can define \textit{Even} using replication. This is given as follows:
    \begin{align*}
        \textit{Even} &= !(\textit{even}(a, n).\textit{Cases}(\textit{True}(a), 
            (\nu \ b)(\textit{Not}(a, b) \mid \overline{\textit{even}} \langle b, m \rangle)(n, m)
        ))
    \end{align*}
    The channel where we check is \textit{even}. The value that we are checking is $n$, and the result is outputted into channel $a$.
    % TODO: Even(Z) \to^* True(a), Even(S(Z)) \to^* False

    The example of the even function looks complicated because it combined two ideas:
    \begin{itemize}
        \item the encoding of natural numbers as processes; and
        \item the use of replication to express a recursive function definition.
    \end{itemize}
    We can explain the second point more easily if we assume that we extend $\pi$-calculus with integers, booleans, expressions formed from standard operations, reduction of boolean and integer expressions, and a condition construct
    \[\textit{if } e \textit{ then } P \textit{ else } Q.\]
    It has the following reduction rules:
    \[\textit{if } \texttt{true} \textit{ then } P \textit{ else } Q \to P \qquad \textit{if } \texttt{false} \textit{ then } P \textit{ else } Q \to Q \]
    \[\frac{e \to e'}{\textit{if } e \textit{ then } P \textit{ else } Q \to \textit{if } e' \textit{ then } P \textit{ else } Q}\]
    That way, we can write the \textit{even} process as follows:
    \[\textit{Even} = !(
        \textit{even}(n).(\textit{if IsZero(n) then }\texttt{true} \textit{ else not(Even(n-1))})
    )\]
    We can write the factorial function in a similar manner:
    \[\textit{Fact} = !(
        \textit{fact}(a, n).if isZero(n) then \overline{a} \langle 1 \rangle 
        else (\nu \ b)(\overline{fact} \langle b, n-1 \rangle \mid b(x).\overline{a} \langle n \cdot x \rangle)
    )\]
    % TODO: Check that fact(a, 1) = \overline{a} \langle 1 \rangle \mid Fact

    Note that the functions defined above consume the number they interact with. This makes it impossible to define functions that use their arguments more than once. It is possible to define persistent versions of the booleans and natural numbers, by using replication, e.g. $\textit{True}(a) = !(a(t, f).\overline{t})$.

    Using these examples, and a formal proof, we can see that $\pi$-calculus can express all computable functions. It is possible to prove this by defining a translation from $\lambda$-calculus into $\pi$-calculus, or by directly showing that all recursive functions on natural numbers can be defined in $\pi$-calculus.

    There is an argument that $\pi$-calculus is more fundamental than $\lambda$-calculus because $\pi$-calculus can easily express functional behaviour in terms of communication, whereas modeling concurrency behaviour in $\lambda$-calculus would require elaborate encodings.
    \newpage

    \section{Equivalence of Process}
    Part of the motivation for the topic of process calculus is to be able to reason about \emph{communicating concurrent systems}: to specify how they behave and verify that they behave correctly.

    The first step is to define the syntax and operational semantics (the reduction relation), which we have done. We have also seen how to define some interesting computational scenarios involving concurrency and communication.

    We will now introduce a theory of equivalence of processes, which means equivalence of ongoing interactive behaviour (in contrast, to equivalence of a final result, as in $\lambda$-calculus).

    Previously, we define the processes $\textit{True}(x)$ and $\textit{False}(x)$, and $\textit{Not}(x, y)$. We would like to have a theory in which
    \[(\nu \ y)(\textit{Not}(x, y) \mid \textit{True}(y)) = \textit{False}(x).\]
    The fact that $\textit{False}(x)$ represents the boolean value \textit{false} is to do with the way it interacts on the channel $x$, so the definition of equivalent must take that into account. Moreover, applying \texttt{Not} twice should be the identity function on booleans, i.e.
    \[x(t, f).\overline{y} \langle t, f \rangle = (\nu \ z)(\textit{Not}(x, z) \mid \textit{Not}(z, y))\]

    To develop the theory of equivalence, we will simplify $\pi$-calculus by assuming that all messages are empty. To reduce syntax, we will omit the empty messages and just write $x$ or $\overline{x}$. We will refer to $x$ and $\overline{x}$ as actions. This means that we are working in an earlier process calculus called CCS-calculus of Communication Systems. We will start with examples and then give proper definitions.

    Reductions of the form $P \to Q$ tell us how a process evolves by \emph{internal} communication. We need to be able to describe \emph{potential} interaction between a process and its environment. We do this by introducing \emph{labelled} transitions, e.g.
    \[a.P \xrightarrow{a} P \qquad \overline{a}.Q \xrightarrow{\overline{a}} Q\]
    We can describe actual interactions using $\tau$ transitions, e.g.
    \[a.P \mid \overline{a}.Q \xrightarrow{\tau} P \mid Q\]

    We will now define labelled transitions. Let $\alpha$ range over $\{a, \overline{a}, \tau\}$. The the inference rules for labelled transitions are given as follows:
    \[\overline{\alpha.P \xrightarrow{\alpha} P}\]
    \[\frac{P \xrightarrow{a} P' \qquad Q \xrightarrow{\overline{a}} Q'}{P \mid Q \xrightarrow{\tau} P' \mid Q'} \qquad \frac{P \xrightarrow{\overline{a}} P' \qquad Q \xrightarrow{a} Q'}{P \mid Q \xrightarrow{\tau} P' \mid Q'}\]
    \[\frac{P \xrightarrow{\alpha} P'}{P \mid Q \xrightarrow{\alpha} P' \mid Q} \qquad \frac{Q \xrightarrow{\alpha} Q'}{P \mid Q \xrightarrow{\alpha} P \mid Q'}\]
    \[\frac{P \xrightarrow{\alpha} P'}{P + Q \xrightarrow{\alpha} P' \mid Q} \qquad \frac{Q \xrightarrow{\alpha} Q'}{P + Q \xrightarrow{\alpha} P \mid Q'}\]
    \[\frac{P \xrightarrow{\alpha} P' \qquad \alpha \in \{x, \overline{x}\}}{(\nu \ x)P \xrightarrow{\alpha} (\nu \ x)P'}\]
    \[\frac{P \xrightarrow{\alpha} P'}{!P \xrightarrow{\alpha} P' \mid !P} \qquad \frac{P \xrightarrow{\overline{a}} P' \qquad P \xrightarrow{a} P''}{!P \xrightarrow{\tau} (P' \mid P'') \mid !P}\]

    We can now talk about \emph{traces} of a process, which are its possible sequences of actions. The empty trace $\epsilon$ is always a possibility. We look at some examples:
    \begin{itemize}
        \item $a.b.0$ has traces $\epsilon$, $a$, $ab$;
        \item $a.0 + b.0$ has traces $\epsilon$, $a$, $b$;
        \item $a.\overline{b}.0 + b.0$ has traces $\epsilon$, $a$, $b$, $a\overline{b}$; and
        \item $a.b.0 + b.\overline{c}.0 + \overline{c}.0$ has traces $\epsilon$, $a$, $b$, $\overline{c}$, $ab$, $b\overline{c}$.
    \end{itemize}

    We can show the labelled transitions of a process in a diagram, with or without the process terms. We can then read off the traces. For example, the process $a.\overline{b}.0 + b.0$ corresponds to the following figure:
    % TODO: Add figure

    A simple equivalence between processes is \emph{trace equivalence}. We say that $P =_{tr} Q$ if they have the same traces. For example, $(\nu \ b)(a.\overline{b}.0 \mid b.c.0)$ and $a.\tau.c.0$ are trace equivalent since they have the same traces:
    % TODO: Add figure

    There is an issue with trace equivalence- it is too weak. To see this, consider the two processes $P = a.b.0 + a.c.0$ and $Q = a.(b.0 + c.0)$. They have the following trace diagrams:
    % TODO: Add figure
    Both the processes have traces $\epsilon$, $a$, $ab$ and $ac$. Hence, they are trace equivalent. However, $P$ can go into a state in which only one of $b$ and $c$ is available, whereas $Q$ offers both $b$ and $c$ from the same state. 

    Next, define $R = \overline{a}.\overline{b}.0$. We have that $Q \mid R$ can communicate on $a$ then $b$, and both processes reach $0$. On the other hand, $P \mid R$ can communicate with $a$, with $P$ becoming either $b.0$ or $c.0$. In the second case, no further communication is possible. This is a deadlock. We would like a definition of equivalence in which $P$ and $Q$ are \emph{not} equivalent.
    % TODO: Illustrate using reduction.

    To do this, we define \emph{bisimulation}. This concept is fundamental in process calculus and concurrency theory. There are many forms of it. We look at \emph{strong bisimulation}. The idea is that for processes $P$ and $Q$ to be bisimilar, it must be that:
    \begin{itemize}
        \item for any action $\alpha \in \{a, \overline{a}, \tau\}$, if $P \xrightarrow{\alpha} P'$, then there is a $Q'$ such that $Q \xrightarrow{\alpha} Q'$ and $P'$ and $Q'$ are bisimilar;
        \item this holds the other way as well, i.e. when $Q \xrightarrow{\alpha} Q'$, then there exists a $P'$ bisimilar to $Q'$ such that $P \xrightarrow{\alpha} P'$ as well.
    \end{itemize}

    We now consider the processes $P = (\nu \ b)(a.\overline{b}.0 \mid b.c.0)$ and $Q = a.\tau.c.0$. We previously saw that the two processes were trace equivalent. We will now see that they are also bisimilar, since they evolve in the following manner:
    % TODO: Draw transition figure (and label each one)

    Now, we can work from the end to prove that $P$ and $Q$ are bisimilar. We use the symbol $\sim$ for strong bisimulation. In this case, $P_3 \sim Q_3$ because neither of them has any transitions; they are both structurally congruent to $0$. So, there is nothing to check here. Now, consider $P_2$ and $Q_2$. We have $P_2 \xrightarrow{c} P_3$, which can be matched by $Q_2 \xrightarrow{c} Q_3$, and we know that $P_3 \sim Q_3$. In the other direction, the only transition of $Q_2$ is $Q_2 \xrightarrow{c} Q_3$, which can equally be matched by $P_2 \xrightarrow{c} P_3$, with $P_3 \sim Q_3$.

    Similarly, we can show that $P_1 \sim Q_1$- the only transition of $P_1$ is $P_1 \xrightarrow{\tau} P_2$ from communication on $b$. This can be matched by $Q_1 \xrightarrow{\tau} Q_2$, and we know that $P_2 \sim Q_2$. In the other direction, the only transition of $Q_1$ is $Q_1 \xrightarrow{\tau} Q_2$, which can be matched by $P_1 \xrightarrow{\tau} P_2$, and we know that $P_2 \sim Q_2$.

    Finally, we consider the transitions of $P$ and $Q$ in the same way. We can see that each process has an $a$-transition that can match the $a$-transition of the other process, resulting in $P_1$ and $Q_1$, with $P_1 \sim Q_1$. We conclude that $P \sim Q$.

    Now, consider $P = a.b.0 + a.c.0$ and $Q = a.(b.0 + c.0)$. We saw that $P$ and $Q$ were trace equivalent. However, they are not bisimilar- consider the trace diagram for $P$ and $Q$.
    % TODO: Add figure
    We see that $P$ has two transitions- $P \xrightarrow{a} P_1$ and $P \xrightarrow{a} P_2$. We can match $P \xrightarrow{a} P_1$ by $Q \xrightarrow{a} Q_1$, so we would need $P_1 \sim Q_1$. Also, $P \xrightarrow{a} P_2$ is matched by $Q \xrightarrow{a} Q_1$. But, $Q_1$ has a transition $Q_1 \xrightarrow{b} 0$, which cannot be matched by $P_2$. So, we cannot have $P \sim Q$.

    We have found two processes that are trace equivalent but not strongly bisimilar. Strong bisimulation is a better definition of equivalence when we have non-determinism. This is because it takes into account how a process can interact with other processes. If two processes are strongly bisimilar, then they are also trace equivalent. If two deterministic processes are trace equivalent, then they are also strongly bisimilar.

    Note that the term `strong' means that we include $\tau$ transitions when matching. Sometimes, this is undesirable because $\tau$ transitions are supposed to be internal unobservable actions. There is another form of equivalence, \emph{weak bisimulation}, which ignores $\tau$ transitions.

    We will now aim to define strong bisimulation. A first attempt might be the following:
    \begin{definition}
        We say that $P$ and $Q$ are \emph{strongly bisimilar}, denoted $P \sim Q$ if for every action $\alpha$:
        \begin{itemize}
            \item if $P \xrightarrow{\alpha} P'$, then there is a $Q'$ such that $Q \xrightarrow{\alpha} Q'$ and $P' \sim Q'$; and
            \item if $Q \xrightarrow{\alpha} Q'$, then there is a $P'$ such that $P \xrightarrow{\alpha} P'$ and $P' \sim Q'$.
        \end{itemize}
    \end{definition}
    This definition is recursive in nature, but does not have a base case. Moreover, there is no reason that $P'$ and $Q'$ are 'smaller' than $P$ and $Q$.

    For instance, consider the process $P = !a.0$. We want $P \sim P$. Because $a.0 \xrightarrow{a} 0$, we have
    \[P \xrightarrow{a} 0 \mid !a.0 \equiv 0 \mid P.\]
    Applying the attempted definition of strong bisimulation to $P$ and $P$, we see that they can match each other's actions. Checking the subsequent processes tells us that $P \sim P$ depends on $0 \mid P \sim 0 \mid P$. This is circular and not a valid definition. Moreover, the reduced process is equivalent to itself, meaning that it does not get smaller.

    Instead of defining bisimulation for $\pi$-calculus processes, we can work with more abstract \emph{labelled transition systems} (LTS). An LTS is a directed graph in which edges are labelled with actions. We use the transition notation
    \[m \xrightarrow{a} n\]
    where $m$ and $n$ are nodes in the graph. We can think of an LTS as a transition diagram for processes, in which the nodes have $\pi$-calculus processes associated with them.

    The first step is to consider a general relation $\mathcal{R}$ on processes (or nodes of an LTS). If $P_1$ and $P_2$ are processes related by $\mathcal{R}$, we write $(P_1, P_2) \in \mathcal{R}$ or $P_1 \mathcal{R} P_2$. Now, we define the property of being a strong bisimulation, which is a property that a relation might or might not have.
    \begin{definition}
        A relation $\mathcal{R}$ on processes (or on nodes of an LTS) is a \emph{strong bisimulation} if whenever $P \mathcal{R} Q$, for every action $\alpha$, it is the case that
        \begin{itemize}
            \item if $P \xrightarrow{\alpha} P'$, then there is $Q'$ such that $Q \xrightarrow{\alpha} Q'$, and $P' \mathcal{R} Q'$; and
            \item if $Q \xrightarrow{\alpha} Q'$, then there is $P'$ such that $P \xrightarrow{\alpha} P'$, and $P' \mathcal{R} Q'$.
        \end{itemize}
    \end{definition}

    Consider a labelled transition system as follows.
    % TODO: Show
    \begin{itemize}
        \item The empty relation $\varnothing$ is a strong bisimulation- this is trivially the case.
        \item The relation $\{(P_2, Q_2)\}$ is not a strong bisimulation- we have $P_2 \xrightarrow{b} P_2$, but in $Q_2$, the only $b$-transition is to $Q_3$, but $(P_2, Q_3) \not\in \mathcal{R}$.
        \item The relation $\{(P_2, P_2)\}$ is a strong bisimulation since it can only reduce to itself.
        \item The relation $\{(P_2, Q_2), (P_2, Q_3)\}$ is a strong bisimulation- we have $P_2 \xrightarrow{b} P_2$ matching $Q_2 \xrightarrow{b} Q_3$ and $Q_3 \xrightarrow{b} Q_2$; and $P_2 \xrightarrow{a} P_2$ matching $Q_2 \xrightarrow{a} Q_2$ and $Q_3 \xrightarrow{a} Q_3$.
        \item The relation $\{(P_2, Q_2), (P_2, Q_3), (Q_2, P_2), (Q_3, P_2)\}$ is a strong bisimulation- we have added the symmetrical cases.
        \item The relation $\{(P_1, Q_1), (P_2, Q_2), (P_2, Q_3)\}$ is a strong bisimulation since $P_1 \xrightarrow{c} P_2$ is matched by $Q_1 \xrightarrow{c} Q_2$.
        \item The relation $\{(P_1, Q_1), (P_2, Q_2), (P_2, Q_3), (Q_1, P_1), (Q_2, P_2), (Q_3, P_2)\}$ is a strong bisimulation.
    \end{itemize}

    For a given LTS (or a set of processes), there can be several relations that satisfy the conditions to be a strong bisimulation. We say that \emph{strong bisimilarity}, denoted by $\sim$, is the largest strong bisimulation. We say that processes $P$ and $Q$ are strong bisimilar, denoted $P \sim Q$, if $(P, Q)$ is in the largest strong bisimulation. To prove this, it is sufficient to find any bisimulation containing $(P, Q)$.

    We have previously used inductive definitions, e.g. the definitions of reduction and typing judgments. An inductive definition is when we define a set or relation to be the smallest entity satisfying certain conditions. Ordinary recursive function definitions are also in this category. A \emph{coninductive} definition is when we define a set or a relation to be the largest entity satisfying certain conditions, such as strong bisimilarity.

    % TODO: Exercise => find the largest strong bisimulation in this example!

    We will now look at processes in the syntax $\pi$-calculus, rather than abstract LTS. When defining labelled transitions for $\pi$-calculus, we did not take structural congruence into account. So, if we want to show something like $P \mid Q$ and $Q \mid P$ to be bisimilar, we have to prove it by constructing a strong bisimulation that contains $(P \mid Q, Q \mid P)$. 
    
    \begin{proposition}
        Let $P$ and $Q$ be processes. Then $P \mid Q \sim Q \mid P$.
    \end{proposition}
    \begin{proof}
        To prove this, it suffices to show that there is a relation containing $(P \mid Q, Q \mid P)$. So, define the relation
        \[\mathcal{R} = \{(P \mid Q, Q \mid P) \mid P, Q \textrm{ processes}\}.\]
        We show that $\mathcal{R}$ is a strong bisimulation. So, let $(P \mid Q, Q \mid P) \in \mathcal{R}$ There are 3 transitions for $P \mid Q$:
        \begin{itemize}
            \item First assume that $P \mid Q \xrightarrow{\alpha} P' \mid Q$ because $P \xrightarrow{\alpha} P'$. In that case, we find that $Q \mid P \xrightarrow{\alpha} Q \mid P'$ by definition of parallel composition. We know that $(P' \mid Q, Q \mid P') \in \mathcal{R}$ by definition of $\mathcal{R}$.
            
            \item The same result follows if $P \mid Q \xrightarrow{\alpha} P \mid Q'$ because $Q \xrightarrow{\alpha} Q'$.
            
            \item Finally, assume that $P$ is $a.P_1 + P_2$ and $Q$ is $\overline{a}.Q_1 + Q_2$, and $P \mid Q \xrightarrow{\tau} P_1 \mid Q_1$. Then, we also have $Q \mid P \xrightarrow{\tau} Q_1 \mid P_1$, and we have $(P_1 \mid Q_1, Q_1 \mid P_1) \in \mathcal{R}$.
        \end{itemize}
        So, it follows that $\mathcal{R}$ is a strong bisimulation. Since $(P \mid Q, Q \mid P) \in \mathcal{R}$, we conclude that $P \mid Q \sim Q \mid P$.
    \end{proof}
    \noindent We can do the same thing to show that structural congruence is included within strong bisimulation.
    % TODO: P \mid 0 \sim P

    Now, let $P = (\nu \ b, c)((a.b.0 + c.d.0) \mid \overline{b}.0)$ and $Q = a.\tau.0$. We show that $P$ and $Q$ are strongly bisimilar. In $P$, $b$ and $c$ are private channels, so we cannot transition using these channels. This means that our only option is:
    \[P \xrightarrow{a} P_1 = (\nu \ b, c)(b.0 \mid \overline{b}.0).\]
    Then, we have
    \[P_1 \xrightarrow{\tau} P_2 = (\nu \ b, c)(0 \mid 0).\]
    So, the relation we want is:
    \[\mathcal{R} = \{(P, Q), (P_1, \tau.0), (P_2, 0)\}.\]
    We note that $P_2$ and $0$ are strongly bisimilar since they has no transitions. This means that $\mathcal{R}$ is a strong bisimulation. Hence, $P$ and $Q$ are strongly bisimilar.

    There are many extensions to strong bisimulation, including:
    \begin{itemize}
        \item allowing messages, including mobile names- this requires a more complex version of the labelled transition rules; and
        \item defining \emph{weak bisimulation}, which ignores $\tau$-transitions. This allows internal interaction to be ignored and focuses on observable input-output.
    \end{itemize}
    \newpage

    \section{Types in $\pi$-calculus}
    In general, static typechecking aims to eliminate errors. In terms of $\pi$-calculus, we want to ensure that whenever a message is sent, the sender and receiver agree on its structure. In particular, we would like to eliminate processes such as:
    \begin{itemize}
        \item $\overline{a} \langle u, v \rangle.0 \mid a(x).P$, where the two subprocesses do not agree on the number of parameters; and
        \item $\overline{a} \langle \textit{true}, \textit{false} \rangle.0 \mid a(x, y).\overline{a} \langle x + y \rangle.0$, where the two subprocesses do not agree on type.
    \end{itemize}

    We will now look at \emph{simple} type system for $\pi$-calculus. It deals with these two issues by specifying:
    \begin{itemize}
        \item the number of components of messages on the channel and
        \item the type of each component.
    \end{itemize}
    We illustrate this with an example. So, assume that the channel $a$ is of type $\texttt{Chan}[\textit{int}, \textit{bool}]$, denoted by $a \colon \texttt{Chan}[\textit{int}, \textit{bool}]$. Then,
    \begin{itemize}
        \item $\overline{a} \langle 1 + 2, \textit{true} \rangle$ is well-typed;
        \item $a(x, y).P$ is well-typed if $x \colon \textit{int}$ and $y \colon \textit{bool}$ in $P$; while
        \item $\overline{a} \langle 1 \rangle$, $\overline{a} \langle true, 1, false \rangle$ and $a(x)$ are incorrect.
    \end{itemize}
    Instead, if $a \colon \texttt{Chan}[\texttt{Chan}[\textit{int}]]$. Then, $a(x).x(y).Q$ is well-typed if $y \colon \textit{int}$ in $Q$.

    Formally, types are specified by the following grammar:
    \begin{align*}
        T &:: == \texttt{Chan}[T, \dots, T] \\
        &\mid \textit{bool} \\
        &\mid \dots,
    \end{align*}
    where the final line can be used for any types we would want the $\pi$-calculus to have, e.g. \textit{int}.

    As for $\lambda$-calculus, we modify the syntax of processes so that binding occurences are annotated with types, e.g.
    \[a(x_1 \colon T_1, \dots, x_n \colon T_n) \qquad (\nu \ x \colon T)\]
    
    We will consider \emph{polyadic $\pi$-calculus}. This means that a message has zero or more components. We will also assume that we have an expression language which we will not define precisely, but contains primitives such as integers and booleans. We also assume that the expression language has a type system with types \textit{int} and \textit{bool}, along with a reduction relation.

    Now, we define the modified syntax of $\pi$-calculus, including processes $P$, values $v$ and expressions $e$:
    \begin{align*}
        P &::= 0  & \textrm{terminated process} \\
        &\mid \overline{x} \langle e_1, \dots, e_n \rangle.P & \textrm{output/send} \\
        &\mid x(y_1 \colon T_1, \dots, y_n \colon T_n).P & \textrm{input/receive} \\
        &\mid (\nu  \ x \colon T)P & \textrm{scope/restriction} \\
        &\mid P \mid Q & \textrm{parallel composition} \\
        &\mid P + Q & \textrm{choice} \\
        &\mid \dots \\
        v &::= x & \textrm{channel names} \\
        &\mid \textit{true}, \textit{false} & \textrm{boolean values} \\
        &\mid \dots \\
        e &::= v & \textrm{values} \\
        &\mid e == e & \textrm{equality expressions} \\
        &\mid \dots
    \end{align*}
    The dots allow for expansion of the syntax as required.

    Assuming \emph{call-by-value}, the reduction rule for communication, with polyadic messages, is:
    \begin{align*}
        (x(y_1 \colon T_1, \dots, y_n \colon T_n).P + \dots) &\mid 
        (\overline{x}\langle v_1, \dots, v_n \rangle.P + \dots) \to \\
        P[y_1 := v_1, \dots, y_n := v_n] &\mid Q
    \end{align*}
    If $n = 0$, then we do not send/receive any message- this is synchronisation. If we have a send and a receive on the same channel with different number of components in message, then there is no reduction. This is a communication error. We define the type system in a way that avoids such errors.

    Let $\Gamma = x_1 \colon T_1, \dots, x_n \colon T_n$ be a \emph{typing context or environment}. We have typing judgment for processes and expressions.
    \begin{itemize}
        \item The \emph{typing judgment for processes} is $\Gamma \vdash P$ (i.e. under $\Gamma$, $P$ is a process). A process does not have a type- the judgment just says that it uses channels in $\Gamma$ correctly. 
        \item The \emph{typing judgment for expressions} is $\Gamma \vdash e \colon T$ (i.e. under $\Gamma$, the expression $e$ has type $T$). This is similar to simply typed $\lambda$-calculus. It includes the case where $e$ is just a channel name.
    \end{itemize}
    
    We now define the typing rules. The process $0$ is well-typed in any environment:
    \[\overline{\Gamma \vdash 0} \textrm{TNil}.\]
    Well-typed processes can be put in parallel, or used as alternatives.
    \[\frac{\Gamma \vdash P \qquad \Gamma \vdash Q}{\Gamma \vdash P \mid Q} \textrm{TPar} \qquad \frac{\Gamma \vdash P \qquad \Gamma \vdash Q}{\Gamma \vdash P + Q} \textrm{TPlus}\]
    An output must send a message of the correct structure.
    \[\frac{\Gamma \vdash x \colon \texttt{Chan}[T_1, \dots, T_n] \qquad \Gamma \vdash e_1 \colon T_1, \dots, e_n \colon T_n \qquad \Gamma \vdash P}{\Gamma \vdash \overline{x} \langle e_1, \dots, e_n\rangle.P} \textrm{TOut}\]
    In an input, the message must have the correct structure, and its components must be used correctly.
    \[\frac{\Gamma \vdash x \colon \texttt{Chan}[T_1, \dots, T_n] \qquad \Gamma, y_1 \colon T_1, \dots, y_n \colon T_n \vdash P}{\Gamma \vdash x(y_1 \colon T_1, \dots, y_n \colon T_n).P} \textrm{TIn}\]
    When a new channel is created, it must be used correctly.
    \[\frac{\Gamma, x \colon \texttt{Chan}[T_1, \dots, T_n] \vdash P}{\Gamma \vdash (\nu \ x) \colon \texttt{Chan}[T_1, \dots, T_n] P} \textrm{TNew}\]

    We now illustrate the types with an example. So, consider the following processes:
    \begin{align*}
        \textit{Server} &= a(x).x(y).\overline{x} \langle y + 1 \rangle.0 \\
        \textit{Client} &= (\nu \ c)\overline{a} \langle c \rangle.\overline{c} \langle 2 \rangle.c(x).0
    \end{align*}
    % TODO: Construct $\Gamma$ such that $\Gamma \vdash \textit{Server} \mid \textit{Client}$

    The simple type system for $\pi$-calculus has issues. In particular, 
    \begin{itemize}
        \item deadlock procesess are typeable, e.g. $\overline{b}\langle \textit{true} \rangle.0 \mid c(y \colon \textit{int}).0$; and
        \item recursive processes are not typeable, even though they should be.
    \end{itemize}
    
    Nonetheless, we will show that the simple type system is safe. This can be done by showing:
    \begin{itemize}
        \item type preservation: if $\Gamma \vdash P$ and $P \to Q$, then $\Gamma \vdash Q$;
        \item absence of immediate communication errors: if $\Gamma \vdash P$ at the top level, $P$ does not have an input and an output in parallel on the same channel with different number of message components (and types).
    \end{itemize}
    Proving these theorems allows us to show that well-typed processes do not generate communication errors.

    To prove these theorems, we need some auxiliary results.
    \begin{lemma}[Substitution in Processes]
        If $\Gamma, x_1, \colon T_1, \dots, x_n \colon T_n$ and for each $1 \leq i \leq n$, $\Gamma \vdash v_i \colon T_i$, then 
        \[\Gamma \vdash P[x_1 := v_1, \dots, x_n := v_n].\]
    \end{lemma}
    \noindent If $v_i$ are only allowed to be channel names, then we can prove this by induction on the derivation of $\Gamma \vdash P$.

    Next, to handle values in the expression language, we need a similar lemma for substitution in expressions.
    \begin{lemma}[Substitution in Expressions]
        If $\Gamma, x_1 \colon T_1, \dots, x_n \colon T_n \vdash e \colon T$ and for $1 \leq i \leq n$, $\Gamma \vdash v_i \colon T_i$, then
        \[\Gamma \vdash e[x_1 := v_1, \dots, x_n := v_n] \colon T.\]
    \end{lemma}
    \noindent This can also be proved using induction.

    Because reduction allows the use of structural congruence, we need to prove that this does not affect typeability.
    \begin{lemma}
        If $\Gamma \vdash P$ and $P \equiv Q$, then $\Gamma \vdash Q$.
    \end{lemma}
    \noindent We can prove this by structural induction on $\equiv$. In particular, we consider that $P$ is typeable and then show that $Q$ is typeable, and vice versa.
    \begin{proof}[Proof for scope expansion]
        The rule for structural congruence tells us that for processes $P$ and $Q$, and a channel $x \not\in FV(P)$,
        \[(\nu \ x)(P \mid Q) \equiv P \mid (\nu \ x)Q.\]
        If the left hand side is typeable, then we have
        \[\cfrac{\Gamma, x \colon T \vdash P \qquad \Gamma, x \colon T \vdash Q}{\cfrac{\Gamma, x \colon T \vdash P \mid Q}{\Gamma \vdash (\nu \ x \colon T)(P \mid Q)}}\]
        using TPar and then TNew. Hence, we need $\Gamma, x \colon T \vdash P$ and $\Gamma, x \colon T \vdash Q$ for the left hand side to be typeable. Since $x \not\in FV(P)$, we can show that $\Gamma \vdash P$. Hence,
        \[\cfrac{\Gamma, x \colon T \vdash Q}{\cfrac{\Gamma \vdash P \qquad \Gamma \vdash (\nu \ x \colon T)Q}{\Gamma \vdash P \mid (\nu \ x \colon T)Q}}\]
        using TNew and then TPar. So, if the left hand side is typeable, then so is the right hand side. Using the reverse argument, we can show that if the right hand side is typeable, then so is the left hand side.
    \end{proof}

    We now prove type preservation.
    \begin{theorem}[Type Preservation]
        If $\Gamma \vdash P$ and $P \to Q$, then $\Gamma \vdash Q$.
    \end{theorem}
    We prove this using structural induction on $P \to Q$, by considering each case for the reduction step. In each case, we then analyse the structure of the derivation of $\Gamma \vdash P$ and use the component parts to show that $\Gamma \vdash Q$. The key case is that of communication, where we have
    \[x(y_1 \colon T_1, \dots, y_n \colon T_n).P \mid \overline{x} \langle v_1, \dots, v_n \rangle.Q \to P[y_1 := v_1, \dots, y_n := v_n] \mid Q.\]
    We ignore non-deterministic choice for simplicity. The typing derivation of the left hand side has the following form:
    \[
    \setstackgap{S}{4pt}
    \cfrac{
        \cfrac{\Shortstack[c]
            {
                \Gamma \vdash x \colon \texttt{Chan}[T_1, \dots, T_n] \\
                \Gamma, y_1 \colon T_1, \dots, y_n \colon T_n  \vdash P 
            }
        }{
            \Gamma \vdash x(y_1 \colon T_1, \dots, y_n \colon T_n).P
        } \textrm{TIn} \qquad \cfrac{\Shortstack[c]
            {
                \Gamma \vdash x \colon \texttt{Chan}[T_1, \dots, T_n] \\
                \Gamma \vdash v_1 \colon T_1, \dots, v_n \colon T_n \qquad \Gamma \vdash Q 
            }
        }{
            \Gamma \vdash \overline{x} \langle v_1, \dots, v_n \rangle.Q 
        } \textrm{TOut}
    }{
        \Gamma \vdash x(y_1 \colon T_1, \dots, y_n \colon T_n).P \mid \overline{x} \langle v_1, \dots, v_n \rangle.Q
    } \textrm{TPar}\]
    Using the Substitution Lemma 1 and the typing derivations, we get that
    \[\Gamma \vdash P[y_1 := v_1, \dots, y_n := v_n]\]
    Using TPar, we can derive that
    \[\Gamma \vdash P[y_1 := v_1, \dots, y_n := v_n] \mid Q,\]
    hence the right hand side is well-typed. The other cases use induction in a similar manner.

    We will now give a precise statement that \emph{a well-typed process does not have immediate communication errors}. First, we can use scope expansion to write any process in the form
    \[(\nu \ x_1) \dots (\nu \ x_n)(P_1 \mid \dots \mid P_n),\]
    where immediate communications among the $P_i$ are at the top level and not within $\nu$ operators.
    \begin{theorem}[Absence of immediate communication errors]
        If 
        \[\Gamma \vdash (\nu \ x_1 \colon T_1, \dots, x_k \colon T_k)(x(y_1 \colon U_1, \dots, y_r \colon U_r).P \mid \overline{x} \langle v_1, \dots, v_n \rangle.Q)\]
        then
        \begin{itemize}
            \item $n = r$ and
            \item $\Gamma \vdash v_i \colon U_i$ for all $1 \leq i \leq n$.
        \end{itemize}
    \end{theorem}
    \noindent So, the number of message components is the same as in the input and the output, and the messages/values components are of the types expected by the input.
    \begin{proof}
        The typing derivation must have the form
        \[\Gamma_1 = \Gamma, x_1 \colon T_1, \dots, x_n \colon T_m.\]
        % TODO: Draw the derivation
        Considering $\Gamma \vdash x(y_1 \colon U_1, \dots, y_r \colon U_r).P$, $\Gamma_1$ must contain $x \colon \texttt{Chan}[U_1, \dots, U_r]$. Moreover, since $\Gamma \vdash \overline{x} \langle v_1, \dots, v_n \rangle.Q$, we see that $x \colon \texttt{Chan}[U_1, \dots, U_r]$ in the derivation. Hence, $n = r$ and $\Gamma \vdash v_i \colon U_i$ for $1 \leq i \leq n$.
    \end{proof}

    The simple type system for $\pi$-calculus provides a way of statically checking that communication channels are used consistently throughout a program. This eliminates a certain class of communication error.
    
    Several type systems have been developed for the $\pi$-calculus. Some aim to classify channels in more precise ways (input/output, linear, polymorphic, etc.). Others aim to eliminate more complex behavioural errors, such as deadlocks and livelocks. Lastly, behavioural type systems specify \emph{order} and \emph{sequence} of send and receive.
    
    \subsection{Advanced Types}
    We will now look at an advanced type system for $\pi$-calculus that aims to classify channels in a more precise way.

    A fairly straightforward refinement of the simple type system introduces two new channel type constructors:
    \begin{itemize}
        \item $\texttt{In}[T_1, \dots, T_n]$ is the type of channels that can be used to \emph{input} messages of type $T_1, \dots, T_n$; and
        \item $\texttt{Out}[T_1, \dots, T_n]$ is the type of channels that can be used to \emph{output} messages of type $T_1, \dots, T_n$.
    \end{itemize}
    Now, types are specified by the following grammar:
    \begin{align*}
        T &::= \texttt{Chan}[T_1, \dots, T_n] \\
        &\mid \texttt{In}[T_1, \dots, T_n] \\
        &\mid \texttt{Out}[T_1, \dots, T_n] \\
        &\mid \dots
    \end{align*}
    
    Input/output channel types introduce a notion of \emph{subtyping}. A subtyping relation connects the new constructors and the original constructor:
    \begin{align*}
        \texttt{Chan}[T_1, \dots, T_n] &<: \texttt{In}[T_1, \dots, T_n] \\
        \texttt{Chan}[T_1, \dots, T_n] &<: \texttt{Out}[T_1, \dots, T_n] 
    \end{align*}

    The following defines the subtyping rules in general- we have reflexivity and transitivity, which make it a partial order:
    \[\frac{}{T <: T} \textrm{SRefl} \qquad \frac{R <: S \qquad S <: T}{R <: T} \textrm{STrans}\]
    We also have the rules we saw above:
    \[\frac{}{\texttt{Chan}[T_1, \dots, T_n] <: \texttt{In}[T_1, \dots, T_n]} \textrm{SChanIn}\]
    \[\frac{}{\texttt{Chan}[T_1, \dots, T_n] <: \texttt{Out}[T_1, \dots, T_n]} \textrm{SChanOut}\]
    We have covariance in subtyping with respect to input:
    \[\frac{S_1 <: T_1 \qquad \dots \qquad S_n <: T_n}{\texttt{In}[S_1, \dots, S_n] <: \texttt{In}[T_1, \dots, T_n]} \textrm{SInIn}\]
    In terms of programming, consider the types \textit{num} and its subtype \textit{num}. Then, a channel accepting \textrm{num} can accept a value of type \textit{int}. We have contravariance in subtyping with respect to output:
    \[\frac{T_1 <: S_1 \qquad \dots \qquad T_n <: S_n}{\texttt{Out}[S_1, \dots, S_n] <: \texttt{Out}[T_1, \dots, T_n]} \textrm{SOutSOut}\]
    In terms of programming, a channel that outputs an \textit{int} can be thought of as a channel that outputs \textit{num}. For subtyping with a general channel, we require both covariance and contravariance:
    \[\frac{S_1 <: T_1 \qquad \dots \qquad S_n <: T_n \qquad T_1 <: S_1 \qquad \dots \qquad T_n <: S_n}{\texttt{Chan}[S_1, \dots, S_n] <: \texttt{In}[T_1, \dots, T_n]} \textrm{SChanChan}\]
    To make use of subtyping, the typing judgment can be extended with the subsumption rule:
    \[\frac{\Gamma \vdash x \colon S \qquad S <: T}{\Gamma \vdash T} \textrm{TSub}\]

    An output must send a message of the correct structure on a channel having an output channel type. The inference rule is now:
    \[\frac{\Gamma \vdash x \colon {\color{red}\texttt{Out}[T_1, \dots, T_n]} \qquad \Gamma \vdash e_1 \colon T_1, \dots, e_n \colon T_n \qquad \Gamma \vdash P}{\Gamma \vdash \overline{x} \langle e_1, \dots, e_n \rangle.P} \textrm{TOut}\]
    The change is highlighted in red- we have \texttt{Out} instead of \texttt{Chan}. 

    In an input, the message must have the correct structure and its components must be used correctly.
    \[\frac{\Gamma \vdash x \colon {\color{red}\texttt{In}[T_1, \dots, T_n]} \qquad \Gamma, y_1 \colon T_1, \dots, y_n \colon T_n \vdash P}{\Gamma \vdash x(y_1 \colon T_1, \dots, y_n \colon T_n).P} \textrm{TIn}\]
    A channel can only be created of type \texttt{Chan} in TNew; it cannot be the other way round.

    In addition to capabilities of input/output, channel types can be refined by introducing \emph{multiplicities} for linear and shared channels. \emph{Linearity} restricts channel usage exactly once, in accordance with its capability:
    \begin{itemize}
        \item linear input type $\ell_i[\cdot]$ can only be used once in input;
        \item linear output type $\ell_o[\cdot]$ can only be used once in out; and
        \item linear channel type $\ell_{\textit{Chan}}[\cdot]$ can only be used once in input and output.
    \end{itemize}
    Linear types allow for resource-aware programming and have inspired ownership types and unique types in PLs. The updated types are the following:
    \begin{align*}
        T &::= \texttt{Chan}[T_1, \dots, T_n] \\
        &\mid \ell_i[T_1, \dots, T_n] \\
        &\mid \ell_o[T_1, \dots, T_n] \\
        &\mid \ell_{\textit{Chan}}[T_1, \dots, T_n] \\
        &\mid \dots
    \end{align*}

    In order to respect linearity, a linear type system must keep track of the resources used, without duplicating or discarding any. We need a notion of \emph{type combination} on types, contexts and ultimately on typing rules. We will use the symbol $\uplus$ for the type combination operator, or simply $+$.

    The combination `$\uplus$' of types is a symmetric operation and is defined by the following rules:
    \begin{align*}
        \ell_i[T_1, \dots, T_n] \uplus \ell_o[T_1, \dots, T_n] &\triangleq \ell_{\textit{Chan}}[T_1, \dots, T_n] \\
        \texttt{Chan}[T_1, \dots, T_n] \uplus \texttt{Chan}[T_1, \dots, T_n] &\triangleq \texttt{Chan}[T_1, \dots, T_n] \\
        \textit{bool} \uplus \textit{bool} &\triangleq \textit{bool} \\
        T \uplus T' &\triangleq \texttt{undef} & \textrm{otherwise}.
    \end{align*}
    Now, we extend `$\uplus$' on typing contexts based on these rules:
    \[(\Gamma_1 \uplus \Gamma_2)(x) \triangleq \begin{cases}
        \Gamma_1(x) \uplus \Gamma_2(x) & \textrm{if both } \Gamma_1(x) \textrm{ and } \Gamma_2(x) \textrm{ are defined} \\
        \Gamma_1(x) & \textrm{if } \Gamma_1(x) \textrm{ is defined but } \Gamma_2(x) \textrm{ is not defined} \\
        \Gamma_2(x) & \textrm{if } \Gamma_2(x) \textrm{ is defined but } \Gamma_1(x) \textrm{ is not defined} \\
        \texttt{undef} & \textrm{otherwise}.
    \end{cases}\]
    
    We also add typing rules for linear output and input.
    \[\frac{\Gamma_0 \vdash x \colon {\color{red}\ell_o[T_1, \dots, T_n]} \qquad \Gamma_1 \vdash e_1 \colon T_1 \quad \dots \quad \Gamma_n \colon  e_n \colon T_n \qquad \Gamma_m \vdash P}{\Gamma_0 \uplus \Gamma_i \uplus \Gamma_m \vdash \overline{x} \langle e_1, \dots, e_n \rangle.P} \textrm{TOut}\]
    \[\frac{\Gamma_0 \vdash x \colon {\color{red}\ell_i[T_1, \dots, T_n]} \qquad \Gamma_1, y_1 \colon T_1, \dots, y_n \colon T_n \vdash P}{\Gamma_0 \uplus \Gamma_1 \vdash x(y_1 \colon T_1, \dots, y_n \colon T_n).P} \textrm{TIn}\]
    Also, when a new channel is created, this must be a linear channel.
    \[\frac{\Gamma, x \colon {\color{red} \ell_{\textit{Chan}}[T_1, \dots, T_n]} \vdash P}{\Gamma \vdash (\nu \ x) \colon \ell_{\textit{Chan}}[T_1, \dots, T_n] P} \textrm{TNew}\]

    Linear type systems guarantee that resources will be used exactly once. However, a process can be built in such a way that resources are not used at all! For instance, consider the following linear process:
    \[a \colon \ell_i[\textit{bool}] \vdash (\nu \ b\colon \texttt{Chan}[\textit{bool}] )\overline{b} \langle \textit{false} \rangle.a(x).0\]
    Although this is a well-typed process under the linear type system, it is deadlocked. Further extension to types and type systems for $\pi$-calculus deal with this, along with lock freedom and termination.

\end{document}